"""
å¿«é€Ÿæ£€æŸ¥DINOv2æ•´åˆçš„å®Œæ•´æ€§
è¿è¡Œæ–¹å¼: python tools/check_dinov2_integration.py
"""

import sys
import os
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)

def check_imports():
    """æ£€æŸ¥æ¨¡å—å¯¼å…¥"""
    print_section("1ï¸âƒ£ æ£€æŸ¥æ¨¡å—å¯¼å…¥")
    
    checks = []
    
    # æ£€æŸ¥DINOv2 Adapter
    try:
        from models.backbones import DinoAdapter
        checks.append(("DinoAdapterå¯¼å…¥", True, "âœ…"))
        print("âœ… DinoAdapterå¯æˆåŠŸå¯¼å…¥")
    except Exception as e:
        checks.append(("DinoAdapterå¯¼å…¥", False, f"âŒ {e}"))
        print(f"âŒ DinoAdapterå¯¼å…¥å¤±è´¥: {e}")
    
    # æ£€æŸ¥RaCFormer
    try:
        from models.racformer import RaCFormer
        checks.append(("RaCFormerå¯¼å…¥", True, "âœ…"))
        print("âœ… RaCFormerå¯æˆåŠŸå¯¼å…¥")
    except Exception as e:
        checks.append(("RaCFormerå¯¼å…¥", False, f"âŒ {e}"))
        print(f"âŒ RaCFormerå¯¼å…¥å¤±è´¥: {e}")
    
    # æ£€æŸ¥RaCFormer_head
    try:
        from models.racformer_head import RaCFormer_head
        checks.append(("RaCFormer_headå¯¼å…¥", True, "âœ…"))
        print("âœ… RaCFormer_headå¯æˆåŠŸå¯¼å…¥")
    except Exception as e:
        checks.append(("RaCFormer_headå¯¼å…¥", False, f"âŒ {e}"))
        print(f"âŒ RaCFormer_headå¯¼å…¥å¤±è´¥: {e}")
    
    return all(c[1] for c in checks)

def check_dinov2_adapter():
    """æ£€æŸ¥DINOv2 AdapteråŠŸèƒ½"""
    print_section("2ï¸âƒ£ æ£€æŸ¥DINOv2 AdapteråŠŸèƒ½")
    
    # æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nå½“å‰è®¾å¤‡: {device}")
    
    if device.type != "cuda":
        print("âš ï¸  è­¦å‘Š: MSDeformAttn ä»…æ”¯æŒ CUDAï¼Œè·³è¿‡å‰å‘ä¼ æ’­æµ‹è¯•")
        print("   è¯·åœ¨ GPU ç¯å¢ƒä¸‹è¿è¡Œå®Œæ•´æµ‹è¯•")
        return False
    
    try:
        from models.backbones import DinoAdapter
        
        # æµ‹è¯•ViT-Smallé…ç½®
        print("\næµ‹è¯• ViT-Small é…ç½®...")
        adapter_small = DinoAdapter(
            num_heads=6,
            embed_dim=384,
            depth=12,
            pretrained_vit=False,  # è·³è¿‡æƒé‡åŠ è½½ä»¥åŠ å¿«æµ‹è¯•
            freeze_dino=True,
        ).to(device)  # ç§»åŠ¨åˆ° GPU
        print(f"  âœ… ViT-Smallåˆå§‹åŒ–æˆåŠŸ")
        print(f"  - embed_dim: {adapter_small.embed_dim}")
        print(f"  - num_heads: {adapter_small.num_heads}")
        print(f"  - depth: {len(adapter_small.blocks)}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
        dummy_input = torch.randn(2, 3, 256, 704, device=device)  # [B, C, H, W] åœ¨ GPU ä¸Š
        with torch.no_grad():
            features, x_out = adapter_small(dummy_input)
        
        print(f"  âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  - è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
        print(f"  - è¾“å‡ºç‰¹å¾æ•°é‡: {len(features)}")
        for i, feat in enumerate(features):
            print(f"  - ç‰¹å¾{i+1}å½¢çŠ¶: {feat.shape}")
        print(f"  - x_outå½¢çŠ¶: {x_out.shape}")
        
        # éªŒè¯batchç»´åº¦
        assert features[0].shape[0] == 2, "âŒ Batchç»´åº¦ä¸¢å¤±ï¼"
        print("  âœ… Batchç»´åº¦ä¿æŒæ­£ç¡®")
        
        # éªŒè¯é€šé“æ•°
        assert features[0].shape[1] == 384, f"âŒ é€šé“æ•°é”™è¯¯ï¼šæœŸæœ›384ï¼Œå®é™…{features[0].shape[1]}"
        print("  âœ… è¾“å‡ºé€šé“æ•°æ­£ç¡® (384)")
        
        return True
        
    except Exception as e:
        print(f"âŒ DINOv2 Adapteræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_semantic_fusion():
    """æ£€æŸ¥è¯­ä¹‰èåˆå±‚"""
    print_section("3ï¸âƒ£ æ£€æŸ¥è¯­ä¹‰èåˆå±‚")
    
    try:
        import torch.nn as nn
        from mmcv.cnn import ConvModule
        
        # æ¨¡æ‹ŸResNet50è¾“å‡ºé€šé“
        resnet_channels = [256, 512, 1024, 2048]
        dinov2_embed_dim = 768
        
        print(f"\nResNet50é€šé“æ•°: {resnet_channels}")
        print(f"DINOv2 embed_dim: {dinov2_embed_dim}")
        
        # åˆ›å»ºèåˆå±‚
        semantic_fusion = nn.ModuleList([
            ConvModule(
                in_channels=resnet_channels[i] + dinov2_embed_dim,
                out_channels=resnet_channels[i],
                kernel_size=1,
                conv_cfg=dict(type='Conv2d'),
                norm_cfg=dict(type='BN2d'),
                bias='auto'
            ) for i in range(4)
        ])
        
        print(f"\nâœ… è¯­ä¹‰èåˆå±‚åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ¯ä¸ªå±‚çº§çš„èåˆ
        print("\næµ‹è¯•å„å±‚çº§èåˆ:")
        batch_size = 2
        H, W = 64, 176  # ç¤ºä¾‹å°ºå¯¸
        
        for i in range(4):
            # æ¨¡æ‹ŸResNetå’ŒDINOv2ç‰¹å¾
            resnet_feat = torch.randn(batch_size, resnet_channels[i], H//(2**i), W//(2**i))
            dinov2_feat = torch.randn(batch_size, dinov2_embed_dim, H//(2**i), W//(2**i))
            
            # æ‹¼æ¥
            combined = torch.cat([resnet_feat, dinov2_feat], dim=1)
            
            # èåˆ
            with torch.no_grad():
                fused = semantic_fusion[i](combined)
            
            print(f"  å±‚çº§{i+1}:")
            print(f"    - ResNetç‰¹å¾: {resnet_feat.shape}")
            print(f"    - DINOv2ç‰¹å¾: {dinov2_feat.shape}")
            print(f"    - æ‹¼æ¥å: {combined.shape}")
            print(f"    - èåˆå: {fused.shape}")
            
            # éªŒè¯è¾“å‡ºå½¢çŠ¶
            assert fused.shape == resnet_feat.shape, f"âŒ èåˆåå½¢çŠ¶ä¸åŒ¹é…ï¼"
            assert fused.shape[1] == resnet_channels[i], f"âŒ è¾“å‡ºé€šé“æ•°é”™è¯¯ï¼"
        
        print("\nâœ… æ‰€æœ‰å±‚çº§èåˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ è¯­ä¹‰èåˆå±‚æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_query_initialization():
    """æ£€æŸ¥æŸ¥è¯¢åˆå§‹åŒ–"""
    print_section("4ï¸âƒ£ æ£€æŸ¥åœ†å½¢çº¿æ€§å€å¢æŸ¥è¯¢åˆå§‹åŒ–")
    
    try:
        from models.racformer_head import RaCFormer_head
        
        # åˆ›å»ºheadï¼ˆä½¿ç”¨ä¸åŸå§‹é…ç½®å…¼å®¹çš„å‚æ•°ï¼‰
        head = RaCFormer_head(
            num_classes=10,
            in_channels=256,
            num_clusters=6,
            num_query=900,
            embed_dims=256,
            code_size=10,
            code_weights=[1.0] * 10,
            # æ·»åŠ  dummy transformer é…ç½®ä»¥é€šè¿‡çˆ¶ç±»åˆå§‹åŒ–æ£€æŸ¥
            transformer=dict(
                type='Transformer',  # ä½¿ç”¨ç®€å•é…ç½®å³å¯ï¼Œæ­¤å¤„ä»…æ£€æŸ¥åˆå§‹åŒ–é€»è¾‘
                act_cfg=dict(type='ReLU', inplace=True),
            ),
            # ä½¿ç”¨ä¸å¸¦ bg_cls_weight çš„ loss_cls é…ç½®
            loss_cls=dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                loss_weight=2.0
            ),
            loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        )
        
        print(f"\nâœ… RaCFormer_headåˆ›å»ºæˆåŠŸ")
        print(f"  - num_query: {head.num_query}")
        print(f"  - num_clusters: {head.num_clusters}")
        
        # æ£€æŸ¥æŸ¥è¯¢bboxåˆå§‹åŒ–
        query_bbox = head.init_query_bbox.weight
        print(f"\næŸ¥è¯¢bboxå½¢çŠ¶: {query_bbox.shape}")
        
        # æå–thetaå’Œdistance
        query_pos = query_bbox[:, :2]
        distances = query_pos[:, 1]
        
        # ç»Ÿè®¡æ¯ä¸ªåœ†ç¯çš„æŸ¥è¯¢æ•°é‡
        unique_dists, counts = torch.unique(distances, return_counts=True)
        
        print(f"\næŸ¥è¯¢åˆ†å¸ƒ:")
        for i, (d, c) in enumerate(zip(unique_dists, counts)):
            print(f"  åœ†{i+1}: è·ç¦»={d:.3f}, æŸ¥è¯¢æ•°={c.item():4d}")
        
        # éªŒè¯çº¿æ€§é€’å¢
        counts_list = counts.tolist()
        is_increasing = all(counts_list[i] <= counts_list[i+1] for i in range(len(counts_list)-1))
        
        if is_increasing:
            print("\nâœ… æŸ¥è¯¢æ•°é‡çº¿æ€§é€’å¢éªŒè¯é€šè¿‡")
        else:
            print("\nâŒ æŸ¥è¯¢æ•°é‡æœªå®ç°çº¿æ€§é€’å¢")
            return False
        
        # éªŒè¯æ€»æ•°
        total = sum(counts_list)
        if total == head.num_query:
            print(f"âœ… æ€»æŸ¥è¯¢æ•°éªŒè¯é€šè¿‡: {total}/{head.num_query}")
        else:
            print(f"âŒ æ€»æŸ¥è¯¢æ•°ä¸åŒ¹é…: {total}/{head.num_query}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_config_files():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
    print_section("5ï¸âƒ£ æ£€æŸ¥é…ç½®æ–‡ä»¶")
    
    config_files = [
        'configs/racformer_r50_nuimg_704x256_f8_with_dinov2.py',
        'configs/racformer_r50_nuimg_704x256_f8.py',
    ]
    
    all_exist = True
    for config_file in config_files:
        if os.path.exists(config_file):
            print(f"âœ… {config_file} å­˜åœ¨")
            
            # å°è¯•è¯»å–é…ç½®
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if 'dinov2_adapter' in content:
                        print(f"   - åŒ…å«DINOv2é…ç½® âœ…")
                    if 'num_clusters' in content:
                        print(f"   - åŒ…å«æŸ¥è¯¢åˆå§‹åŒ–é…ç½® âœ…")
            except Exception as e:
                print(f"   - è¯»å–å¤±è´¥: {e}")
        else:
            print(f"âŒ {config_file} ä¸å­˜åœ¨")
            all_exist = False
    
    return all_exist

def check_weight_paths():
    """æ£€æŸ¥DINOv2æƒé‡è·¯å¾„"""
    print_section("6ï¸âƒ£ æ£€æŸ¥DINOv2æƒé‡è·¯å¾„")
    
    weight_paths = [
        'weight/dinov2_vitb14_pretrain.pth',
        'weight/dinov2_vits14_pretrain.pth',
        'pretrain/dinov2_vitb14_pretrain.pth',
        'pretrain/dinov2_vits14_pretrain.pth',
        os.path.expanduser('~/.cache/dinov2/dinov2_vitb14_pretrain.pth'),
        os.path.expanduser('~/.cache/dinov2/dinov2_vits14_pretrain.pth'),
    ]
    
    found_weights = []
    for path in weight_paths:
        if os.path.exists(path):
            size_mb = os.path.getsize(path) / (1024 * 1024)
            print(f"âœ… æ‰¾åˆ°æƒé‡: {path} ({size_mb:.1f} MB)")
            found_weights.append(path)
    
    if not found_weights:
        print("âš ï¸  æœªæ‰¾åˆ°æœ¬åœ°DINOv2æƒé‡æ–‡ä»¶")
        print("   ä»£ç å°†è‡ªåŠ¨ä»torch.hubä¸‹è½½")
        print("   å»ºè®®æ‰‹åŠ¨ä¸‹è½½åˆ°ä»¥ä¸‹è·¯å¾„ä¹‹ä¸€:")
        print("   - weight/dinov2_vitb14_pretrain.pth")
        print("   - pretrain/dinov2_vitb14_pretrain.pth")
        return False
    
    return True

def generate_summary(results):
    """ç”Ÿæˆæ£€æŸ¥æ€»ç»“"""
    print_section("ğŸ“‹ æ£€æŸ¥æ€»ç»“")
    
    print("\næ£€æŸ¥é¡¹ç›®:")
    for name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    all_critical_passed = all([
        results.get('æ¨¡å—å¯¼å…¥', False),
        results.get('DINOv2åŠŸèƒ½', False),
        results.get('è¯­ä¹‰èåˆ', False),
        results.get('æŸ¥è¯¢åˆå§‹åŒ–', False),
    ])
    
    print("\n" + "=" * 80)
    if all_critical_passed:
        print("ğŸ‰ æ‰€æœ‰å…³é”®æ£€æŸ¥é€šè¿‡ï¼ä»£ç å¯ä»¥è¿è¡Œã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. å¦‚æœè¿˜æ²¡æœ‰DINOv2æƒé‡ï¼Œè¿è¡Œè®­ç»ƒæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½")
        print("  2. è¿è¡ŒæŸ¥è¯¢åˆå§‹åŒ–å¯è§†åŒ–:")
        print("     python tools/verify_query_initialization.py")
        print("  3. å¼€å§‹è®­ç»ƒ:")
        print("     bash tools/dist_train.sh configs/racformer_r50_nuimg_704x256_f8_with_dinov2.py 8")
    else:
        print("âš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„è¯¦ç»†ä¿¡æ¯ã€‚")
    print("=" * 80)

if __name__ == '__main__':
    print("=" * 80)
    print("  RaCFormer + DINOv2 æ•´åˆå®Œæ•´æ€§æ£€æŸ¥")
    print("=" * 80)
    
    results = {}
    
    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
    results['æ¨¡å—å¯¼å…¥'] = check_imports()
    results['DINOv2åŠŸèƒ½'] = check_dinov2_adapter()
    results['è¯­ä¹‰èåˆ'] = check_semantic_fusion()
    results['æŸ¥è¯¢åˆå§‹åŒ–'] = check_query_initialization()
    results['é…ç½®æ–‡ä»¶'] = check_config_files()
    results['æƒé‡æ–‡ä»¶'] = check_weight_paths()
    
    # ç”Ÿæˆæ€»ç»“
    generate_summary(results)

